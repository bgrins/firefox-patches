diff --git a/tools/tryselect/selectors/perf.py b/tools/tryselect/selectors/perf.py
--- a/tools/tryselect/selectors/perf.py
+++ b/tools/tryselect/selectors/perf.py
@@ -25,10 +25,11 @@
     fzf_bootstrap,
     run_fzf,
     setup_tasks_for_fzf,
 )
 from .compare import CompareParser
+from .perfcomparators import get_comparator
 
 here = os.path.abspath(os.path.dirname(__file__))
 build = MozbuildObject.from_environment(cwd=here)
 cache_file = pathlib.Path(build.statedir, "try_perf_revision_cache.json")
 
@@ -547,11 +548,11 @@
                 "type": str,
                 "default": None,
                 "help": "See --browsertime-upload-apk. This option does the same "
                 "thing except it's for mozperftest tests such as the startup ones. "
                 "Note that those tests only exist through --show-all, as they "
-                "aren't contained in any existing categories. ",
+                "aren't contained in any existing categories.",
             },
         ],
         [
             ["--detect-changes"],
             {
@@ -559,10 +560,32 @@
                 "default": False,
                 "help": "Adds a task that detects performance changes using MWU.",
             },
         ],
         [
+            ["--comparator"],
+            {
+                "type": str,
+                "default": "BasePerfComparator",
+                "help": "Either a path to a file to setup a custom comparison, "
+                "or a builtin name. See the Firefox source docs for mach try perf for "
+                "examples of how to build your own, along with the interface.",
+            },
+        ],
+        [
+            ["--comparator-args"],
+            {
+                "nargs": "*",
+                "type": str,
+                "default": [],
+                "dest": "comparator_args",
+                "help": "Arguments provided to the base, and new revision setup stages "
+                "of the comparator.",
+                "metavar": "ARG=VALUE",
+            },
+        ],
+        [
             ["--variants"],
             {
                 "nargs": "*",
                 "type": str,
                 "default": [BASE_CATEGORY_NAME],
@@ -1200,10 +1223,22 @@
         for category, category_info in categories.items():
             PerfParser._handle_negations(category, category_info, **kwargs)
 
         return categories
 
+    def inject_change_detector(base_cmd, all_tasks, selected_tasks):
+        query = "'perftest 'mwu 'detect"
+        mwu_task = PerfParser.get_tasks(base_cmd, [], query, all_tasks)
+
+        if len(mwu_task) > 1 or len(mwu_task) == 0:
+            raise InvalidRegressionDetectorQuery(
+                f"Expected 1 task from change detector "
+                f"query, but found {len(mwu_task)}"
+            )
+
+        selected_tasks |= set(mwu_task)
+
     def check_cached_revision(base_commit=None):
         """
         If the base_commit parameter does not exist, remove expired cache data.
         Cache data format:
         {
@@ -1261,12 +1296,27 @@
             cache_data[base_commit] = new_revision
 
         with cache_file.open(mode="w") as f:
             json.dump(cache_data, f, indent=4)
 
+    def setup_try_config(try_config, extra_args):
+        if try_config is None:
+            try_config = {}
+        if extra_args:
+            args = " ".join(extra_args)
+            try_config.setdefault("env", {})["PERF_FLAGS"] = args
+
     def perf_push_to_try(
-        selected_tasks, selected_categories, queries, try_config, dry_run, single_run
+        selected_tasks,
+        selected_categories,
+        queries,
+        try_config,
+        dry_run,
+        single_run,
+        extra_args,
+        comparator,
+        comparator_args,
     ):
         """Perf-specific push to try method.
 
         This makes use of logic from the CompareParser to do something
         very similar except with log redirection. We get the comparison
@@ -1283,55 +1333,74 @@
         msg = "Perf selections={} (queries={})".format(
             ",".join(selected_categories),
             "&".join([q for q in queries if q is not None and len(q) > 0]),
         )
 
-        updated = False
+        # Get the comparator to run
+        comparator_klass = get_comparator(comparator)
+        comparator_obj = comparator_klass(
+            vcs, compare_commit, current_revision_ref, comparator_args
+        )
+        base_comparator = True if comparator_klass == "BasePerfComparator" else False
+
         new_revision_treeherder = ""
         base_revision_treeherder = ""
         try:
+
             # redirect_stdout allows us to feed each line into
             # a processor that we can use to catch the revision
             # while providing real-time output
             log_processor = LogProcessor()
 
             # Push the base revision first. This lets the new revision appear
             # first in the Treeherder view, and it also lets us enhance the new
             # revision with information about the base run.
-            base_revision_treeherder = PerfParser.check_cached_revision(compare_commit)
+            base_revision_treeherder = None
+            if base_comparator:
+                # Don't cache the base revision when a custom comparison is being performed
+                # since the base revision is now unique and not general to all pushes
+                base_revision_treeherder = PerfParser.check_cached_revision(compare_commit)
+
             if not (dry_run or single_run or base_revision_treeherder):
-                vcs.update(compare_commit)
-                updated = True
+                # Setup the base revision, and try config. This lets us change the options
+                # we run the tests with through the PERF_FLAGS environment variable.
+                base_try_config = copy.deepcopy(try_config)
+                comparator_obj.setup_base_revision(extra_args)
+                PerfParser.setup_try_config(base_try_config, extra_args)
 
                 with redirect_stdout(log_processor):
                     # XXX Figure out if we can use the `again` selector in some way
                     # Right now we would need to modify it to be able to do this.
                     # XXX Fix up the again selector for the perf selector (if it makes sense to)
                     push_to_try(
                         "perf-again",
                         "{msg}".format(msg=msg),
                         try_task_config=generate_try_task_config(
-                            "fuzzy", selected_tasks, try_config
+                            "fuzzy", selected_tasks, base_try_config
                         ),
                         stage_changes=False,
                         dry_run=dry_run,
                         closed_tree=False,
                         allow_log_capture=True,
                     )
 
                 base_revision_treeherder = log_processor.revision
-                PerfParser.save_revision_treeherder(
-                    compare_commit, base_revision_treeherder
-                )
+                if base_comparator:
+                    PerfParser.save_revision_treeherder(
+                        compare_commit, base_revision_treeherder
+                    )
 
                 # Reset updated since we no longer need to worry
                 # about failing while we're on a base commit
-                updated = False
                 try_config.setdefault("env", {})[
                     "PERF_BASE_REVISION"
                 ] = base_revision_treeherder
-                vcs.update(current_revision_ref)
+
+                comparator_obj.teardown_base_revision()
+
+            comparator_obj.setup_new_revision(extra_args)
+            PerfParser.setup_try_config(try_config, extra_args)
 
             with redirect_stdout(log_processor):
                 push_to_try(
                     "perf",
                     "{msg}".format(msg=msg),
@@ -1344,29 +1413,17 @@
                     closed_tree=False,
                     allow_log_capture=True,
                 )
 
             new_revision_treeherder = log_processor.revision
+            comparator_obj.teardown_new_revision()
 
         finally:
-            if updated:
-                vcs.update(current_revision_ref)
+            comparator_obj.teardown()
 
         return base_revision_treeherder, new_revision_treeherder
 
-    def inject_change_detector(base_cmd, all_tasks, selected_tasks):
-        query = "'perftest 'mwu 'detect"
-        mwu_task = PerfParser.get_tasks(base_cmd, [], query, all_tasks)
-
-        if len(mwu_task) > 1 or len(mwu_task) == 0:
-            raise InvalidRegressionDetectorQuery(
-                f"Expected 1 task from change detector "
-                f"query, but found {len(mwu_task)}"
-            )
-
-        selected_tasks |= set(mwu_task)
-
     def run(
         update=False,
         show_all=False,
         parameters=None,
         try_config=None,
@@ -1417,23 +1474,20 @@
             return None
 
         if detect_changes:
             PerfParser.inject_change_detector(base_cmd, all_tasks, selected_tasks)
 
-        if try_config is None:
-            try_config = {}
-        if kwargs.get("extra_args", []):
-            args = " ".join(kwargs["extra_args"])
-            try_config.setdefault("env", {})["PERF_FLAGS"] = args
-
         return PerfParser.perf_push_to_try(
             selected_tasks,
             selected_categories,
             queries,
             try_config,
             dry_run,
             single_run,
+            kwargs.get("extra_args", []),
+            kwargs.get("comparator"),
+            kwargs.get("comparator_args", []),
         )
 
     def run_category_checks():
         # XXX: Add a jsonschema check for the category definition
         # Make sure the queries don't specify variants in them
diff --git a/tools/tryselect/selectors/perfcomparators.py b/tools/tryselect/selectors/perfcomparators.py
new file mode 100644
--- /dev/null
+++ b/tools/tryselect/selectors/perfcomparators.py
@@ -0,0 +1,220 @@
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this file,
+# You can obtain one at http://mozilla.org/MPL/2.0/.
+
+import importlib
+import inspect
+import pathlib
+
+BUILTIN_COMPARATORS = {}
+
+
+class ComparatorNotFound(Exception):
+    """Raised when we can't find the specified comparator.
+
+    Triggered when either the comparator name is incorrect for a builtin one,
+    or when a path to a specified comparator cannot be found.
+    """
+
+    pass
+
+
+class GithubRequestFailure(Exception):
+    """Raised when we hit a failure during PR link parsing."""
+
+    pass
+
+
+class BadComparatorArgs(Exception):
+    """Raised when the args given to the comparator are incorrect."""
+
+    pass
+
+
+def comparator(comparator_klass):
+    BUILTIN_COMPARATORS[comparator_klass.__name__] = comparator_klass
+    return comparator_klass
+
+
+@comparator
+class BasePerfComparator:
+    def __init__(self, vcs, compare_commit, current_revision_ref, comparator_args):
+        self.vcs = vcs
+        self.compare_commit = compare_commit
+        self.current_revision_ref = current_revision_ref
+        self.comparator_args = comparator_args
+
+        # Used to ensure that the local repo gets cleaned up appropriately on failures
+        self._updated = False
+
+    def setup_base_revision(self, extra_args):
+        self.vcs.update(self.compare_commit)
+        self._updated = True
+
+    def teardown_base_revision(self):
+        if self._updated:
+            self.vcs.update(self.current_revision_ref)
+            self._updated = False
+
+    def setup_new_revision(self):
+        pass
+
+    def teardown_new_revision(self):
+        pass
+
+    def teardown(self):
+        self.teardown_base_revision()
+
+
+def get_github_pull_request_info(link):
+    """Returns information about a PR link.
+
+    This method accepts a Github link in either of these formats:
+        https://github.com/mozilla-mobile/firefox-android/pull/1627,
+        https://github.com/mozilla-mobile/firefox-android/pull/1876/commits/17c7350cc37a4a85cea140a7ce54e9fd037b5365 #noqa
+
+    and returns the Github link, branch, and revision of the commit.
+    """
+    from urllib.parse import urlparse
+
+    import requests
+
+    # Parse the url, and get all the necessary info
+    parsed_url = urlparse(link)
+    path_parts = parsed_url.path.strip("/").split("/")
+    owner, repo = path_parts[0], path_parts[1]
+    link_specifier = path_parts[-1]
+
+    if "/pull/" not in parsed_url.path:
+        raise GithubRequestFailure(
+            f"Link for Github PR is invalid (missing /pull/): {link}"
+        )
+
+    # Get the commit being targeted in the PR
+    pr_commit = None
+    if "/commits/" in parsed_url.path:
+        pr_commit = path_parts[-1]
+        link_specifier = path_parts[-3]
+
+    # Make the request, and get the PR info, otherwise,
+    # raise an exception if the response code is not 200
+    api_url = f"https://api.github.com/repos/{owner}/{repo}/pulls/{link_specifier}"
+    response = requests.get(api_url)
+    if response.status_code == 200:
+        link_info = response.json()
+        return (
+            link_info["head"]["repo"]["html_url"],
+            pr_commit if pr_commit else link_info["head"]["sha"],
+            link_info["head"]["ref"],
+        )
+
+    raise GithubRequestFailure(
+        f"The following url returned a non-200 status code: {api_url}"
+    )
+
+
+@comparator
+class BenchmarkComparator(BasePerfComparator):
+    def _get_benchmark_info(self, arg_prefix):
+        # Get the flag from the comparator args
+        benchmark_info = {"repo": None, "branch": None, "revision": None, "link": None}
+        for arg in self.comparator_args:
+            if arg.startswith(arg_prefix):
+                _, settings = arg.split(arg_prefix)
+                setting, val = settings.split("=")
+                benchmark_info[setting] = val
+
+        # Parse the link for any required information
+        if benchmark_info.get("link", None) is not None:
+            (
+                benchmark_info["repo"],
+                benchmark_info["revision"],
+                benchmark_info["branch"],
+            ) = get_github_pull_request_info(benchmark_info["link"])
+
+        return benchmark_info
+
+    def _setup_benchmark_args(self, extra_args, benchmark_info):
+        # Setup the arguments for Raptor
+        extra_args.append(f"benchmark-repository={benchmark_info['repo']}")
+        extra_args.append(f"benchmark-revision={benchmark_info['revision']}")
+
+        if benchmark_info.get("branch", None):
+            extra_args.append(f"benchmark-branch={benchmark_info['branch']}")
+
+    def setup_base_revision(self, extra_args):
+        """Sets up the options for a base benchmark revision run.
+
+        Checks for a `base-link` in the
+        command and adds the appropriate commands to the extra_args
+        which will be added to the PERF_FLAGS environment variable.
+
+        If that isn't provided, then you must provide the repo, branch,
+        and revision directly through these (branch is optional):
+
+            base-repo=https://github.com/mozilla-mobile/firefox-android
+            base-branch=main
+            base-revision=17c7350cc37a4a85cea140a7ce54e9fd037b5365
+
+        Otherwise, we'll use the default mach try perf
+        base behaviour.
+
+        TODO: Get the information automatically from a commit link. Github
+        API doesn't provide the branch name from a link like that.
+        """
+        base_info = self._get_benchmark_info("base-")
+
+        # If no options were provided, use the default BasePerfComparator behaviour
+        if not any(v is not None for v in base_info.values()):
+            raise BadComparatorArgs(
+                f"Could not find the correct base-revision arguments in: {self.comparator_args}"
+            )
+
+        self._setup_benchmark_args(extra_args, base_info)
+
+    def setup_new_revision(self, extra_args):
+        """Sets up the options for a new benchmark revision run.
+
+        Same as `setup_base_revision`, except it uses
+         `new-` as the prefix instead of `base-`.
+        """
+        new_info = self._get_benchmark_info("new-")
+
+        # If no options were provided, use the default BasePerfComparator behaviour
+        if not any(v is not None for v in new_info.values()):
+            raise BadComparatorArgs(
+                f"Could not find the correct new-revision arguments in: {self.comparator_args}"
+            )
+
+        self._setup_benchmark_args(extra_args, new_info)
+
+
+def get_comparator(comparator):
+    if comparator in BUILTIN_COMPARATORS:
+        return BUILTIN_COMPARATORS[comparator]
+
+    file = pathlib.Path(comparator)
+    if not file.exists():
+        raise ComparatorNotFound(
+            f"Expected either a path to a file containing a comparator, or a "
+            f"builtin comparator from this list: {BUILTIN_COMPARATORS.keys()}"
+        )
+
+    # Importing a source file directly
+    spec = importlib.util.spec_from_file_location(name=file.name, location=comparator)
+    module = importlib.util.module_from_spec(spec)
+    spec.loader.exec_module(module)
+
+    members = inspect.getmembers(
+        module, lambda c: inspect.isclass(c) and issubclass(c, BasePerfComparator)
+    )
+
+    if not members:
+        raise ComparatorNotFound(
+            f"The path {comparator} was found but it was not a valid comparator. "
+            f"Ensure it is a subclass of BasePerfComparator and optionally contains the  "
+            f"following methods: "
+            f"{', '.join(inspect.getmembers(BasePerfComparator, predicate=inspect.ismethod))}"
+        )
+
+    return members[0][-1]

